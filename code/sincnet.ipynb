{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"sincnet.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"jeYpoByresAL","colab_type":"code","colab":{}},"cell_type":"code","source":["!pip install mne\n","\n","from collections import OrderedDict\n","import numpy as np\n","import tensorflow as tf\n","from datetime import datetime\n","from sklearn.metrics import cohen_kappa_score\n","from mne.io import read_raw_edf, find_edf_events\n","import helper_functions as hf\n","from load_data import BCICompetition4Set2A\n","from flip_gradient import flip_gradient"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OPwedSg6sZxN","colab_type":"code","colab":{}},"cell_type":"code","source":["device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"AKWhsJF7sgiD","colab_type":"code","colab":{}},"cell_type":"code","source":["# Data helper functions\n","test_subject = 1\n","test_subject -= 1\n","r = 9\n","\n","def BCI_read_data(filename, labels_filename):    \n","    train_loader = BCICompetition4Set2A(filename=filename, labels_filename=labels_filename)\n","    train_cnt = train_loader.load()\n","    train_cnt = train_cnt.drop_channels(['STI 014', 'EOG-left', 'EOG-central', 'EOG-right'])\n","    assert len(train_cnt.ch_names) == 22\n","    \n","    train_cnt = hf.mne_apply(lambda a: a * 1e6, train_cnt)\n","    train_cnt = hf.mne_apply(lambda a: hf.bandpass_cnt(a, 0, 38.0, train_cnt.info['sfreq'],\n","                               filt_order=3,\n","                               axis=1), train_cnt)\n","    train_cnt = hf.mne_apply(lambda a: hf.exponential_running_standardize(a.T, factor_new=1e-3,\n","                                                  init_block_size=1000,\n","                                                  eps=1e-4).T, train_cnt)\n","    \n","    return train_cnt\n","\n","def BCI_load_all_data():\n","    ival = [-500, 4000]\n","    marker_def = OrderedDict([('Left Hand', [1]), ('Right Hand', [2],),\n","                              ('Foot', [3]), ('Tongue', [4])])\n","    \n","    raw_files = [BCI_read_data(\"A0\" + str(f) + \"T.gdf\", \n","                               \"A0\" + str(f) + \"T.mat\") for f in range(1, r+1)]\n","    \n","    \n","    train_set_list = [hf.create_signal_target_from_raw_mne(raw, marker_def, ival) for raw in raw_files]\n","    \n","    train_set_X = np.concatenate([train_set_list[i].X for i in  range(r) if i!=test_subject])\n","    # train_set_X = np.concatenate([train_set.X for train_set in train_set_list[:-1]])\n","    train_set_y = np.concatenate([train_set_list[i].y for i in  range(r) if i!=test_subject])\n","    # train_set_y = np.concatenate([train_set.y for train_set in train_set_list[:-1]])\n","    train_set_z = np.concatenate([np.zeros((288)) + i for i in range(r-1)])\n","    \n","    indices = np.arange(train_set_y.shape[0])\n","    np.random.shuffle(indices)\n","    \n","    train_set = hf.SignalAndTarget(train_set_X[indices, :, :], train_set_y[indices], train_set_z[indices])\n","    test_set = train_set_list[test_subject]\n","    train_set, valid_set = hf.split_into_two_sets(train_set, first_set_fraction=1-0.2)\n","    \n","    iterator = hf.CropsFromTrialsIterator(batch_size=60, input_time_length=1125, n_preds_per_input=4)\n","    \n","    return iterator, train_set, valid_set, test_set"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8hOXutvcsjCl","colab_type":"code","colab":{}},"cell_type":"code","source":["iterator, train_set, valid_set, test_set = BCI_load_all_data()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"apaNGPLAsneD","colab_type":"code","colab":{}},"cell_type":"code","source":["def sinc(band, t_right, width, sinc_0):\n","  y_right = tf.math.sin(np.pi * 2 * band * t_right) / (np.pi * 2 * band * t_right)\n","  y_left = tf.reverse(y_right, [1])[:, :width // 2 - 1 + width % 2]\n","  y = tf.concat([y_left, sinc_0, y_right], 1)\n","  return y\n","\n","samples = 1125\n","num_filters = 100\n","width = samples // 10 \n","fs = 250\n","\n","n = np.arange(0, width)\n","window = 0.54 - 0.46 * np.cos(2 * np.pi * n / width)\n","t_right = np.arange(1, width // 2 + 1 - width % 2)\n","sinc_0 = np.ones([num_filters, 1])\n","\n","def sincnet_model(x, mode):\n","    low_f = tf.get_variable(\"low_f\", initializer=np.random.uniform(0, fs/2-fs/4, [num_filters, 1]))\n","    band_w = tf.get_variable(\"band_w\", initializer=np.random.uniform(0, fs/4, [num_filters, 1]))\n","\n","    high_f = tf.math.abs(low_f + tf.math.abs(band_w))\n","    low_pass1 = 2 * low_f * sinc(low_f, t_right, width, sinc_0)\n","    low_pass2 = 2 * (high_f + band_w) * sinc(high_f, t_right, width, sinc_0)\n","    band_pass = (low_pass2 - low_pass1)\n","    band_pass = band_pass / tf.math.reduce_max(band_pass)\n","\n","    filters = band_pass * window\n","    filters = tf.transpose(filters)\n","    filters = tf.reshape(filters, (1, filters.shape[0], 1, filters.shape[1]))\n","    filters = tf.dtypes.cast(filters, tf.float32)\n","    \n","    conv1 = tf.layers.conv2d(x, int(22*1.5), (22, 1))\n","    #dropout1 = tf.layers.dropout(conv1, 0.5, training=mode)\n","    batch1 = tf.layers.batch_normalization(conv1, momentum=0.1)\n","    perm1 = tf.transpose(batch1, perm=[0, 3, 2, 1])\n","    activation1 = tf.nn.leaky_relu(perm1)\n","    \n","    features = tf.nn.conv2d(activation1, filters, strides=[1, 1, 1, 1], padding=\"VALID\")\n","    pool = tf.layers.max_pooling2d(inputs=features, pool_size=(12, 13), strides=(1, 1))\n","    batch = tf.layers.batch_normalization(pool, momentum=0.1)\n","    activation = tf.nn.leaky_relu(batch)\n","    #dropout = tf.layers.dropout(activation, 0.5, training=mode)\n","        \n","    return activation"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mjb9Mf9FstGC","colab_type":"code","colab":{}},"cell_type":"code","source":["def detect_subject_model(features, mode):\n","    conv1 = tf.layers.conv2d(inputs=features, filters = 40, kernel_size=(1, 25), dilation_rate=(1, 3), strides=1, activation=None)\n","    \n","    conv2 = tf.layers.conv2d(inputs=conv1,filters = 40, kernel_size=(18, 1), dilation_rate=(1, 1), strides=1, activation=None)\n","    batch2 = tf.layers.batch_normalization(conv2, momentum=0.1)\n","    activation2 = tf.nn.elu(batch2)\n","    pool2 = tf.layers.max_pooling2d(inputs=activation2, pool_size=(1, 60), strides=5)\n","    dropout2 = tf.layers.dropout(pool2, 0.5, training=mode)\n","          \n","    conv3 = tf.layers.conv2d(inputs=dropout2, filters = 8, kernel_size=(1, 30), dilation_rate=(1, 6), strides=1, activation=None)\n","    logits = tf.reshape(conv3, [-1, 8])\n","\n","    return logits, features\n","  \n","def shallow_brain_model(features, sinc_feat, mode):\n","    conv0 = tf.layers.conv2d(inputs=features, filters = num_filters, kernel_size=(1, 124), strides=1, activation=tf.nn.relu)\n","    \n","    if sinc_feat is not None:\n","        features = conv0 - sinc_feat\n","    \n","    conv1 = tf.layers.conv2d(inputs=features, filters = 40, kernel_size=(1, 25), dilation_rate=(1, 3), strides=1, activation=None)\n","    \n","    conv2 = tf.layers.conv2d(inputs=conv1, filters = 40, kernel_size=(18, 1), dilation_rate=(1, 1), strides=1, activation=None)\n","    batch2 = tf.layers.batch_normalization(conv2, momentum=0.1)\n","    activation2 = tf.nn.elu(batch2)\n","    pool2 = tf.layers.max_pooling2d(inputs=activation2, pool_size=(1, 60), strides=5)\n","    dropout2 = tf.layers.dropout(pool2, 0.5, training=mode)\n","    \n","    conv3 = tf.layers.conv2d(inputs=dropout2, filters = 4, kernel_size=(1, 30), dilation_rate=(1, 6), strides=1, activation=None)\n","    logits = tf.reshape(conv3, [-1, 4])\n","    \n","    return logits"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DN4ny6CPt5yH","colab_type":"code","colab":{}},"cell_type":"code","source":["tf.reset_default_graph()\n","\n","mode = tf.placeholder(tf.bool)\n","# mode = True\n","# X = tf.placeholder(tf.float32, shape=[60, 22, 1125, 1])\n","X = tf.placeholder(tf.float32, shape=[None, None, None, 1])\n","# Y= tf.placeholder(tf.float32, shape=[60, 22, 1002, 100])\n","Y = tf.placeholder(tf.float32, shape=[None, None, None, num_filters])\n","labels = tf.placeholder(tf.int32, shape=[None])\n","lr = tf.placeholder(tf.float32)\n","\n","sincnet = sincnet_model(X, mode)\n","subject_model = detect_subject_model(sincnet, mode)\n","\n","class_model = shallow_brain_model(X, Y, mode)\n","\n","print(np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()]))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sE5LvdUlwhaY","colab_type":"code","colab":{}},"cell_type":"code","source":["subject_loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=subject_model[0])\n","class_loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=class_model) \n","\n","subject_loss_summ = tf.summary.scalar(\"Subject_loss\", subject_loss)\n","class_loss_summ = tf.summary.scalar(\"Class_loss\", class_loss)\n","\n","class_optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n","subject_optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n","\n","subject_op = subject_optimizer.minimize(loss=subject_loss, global_step=tf.train.get_global_step())\n","class_op = class_optimizer.minimize(loss=class_loss)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"M9Us9JV8wn1F","colab_type":"code","colab":{}},"cell_type":"code","source":["epoch_number = 35\n","max_test_acc = 0\n","\n","with tf.Session() as sess:\n","  \n","    sess.run(tf.global_variables_initializer())\n","    saver = tf.train.Saver()\n","    \n","    print(\"{} Start training...\".format(datetime.now()))\n","    \n","    for epoch in range(epoch_number):\n","      \n","        all_pred_subject = []\n","        all_loss_subject = []\n","        all_subjects = []\n","        val_pred_subject = []\n","        val_loss_subject = []\n","        val_subjects = []\n","        test_pred_subject = []\n","        \n","        \n","        all_pred_class = []\n","        all_loss_class = []\n","        all_targets = []\n","        val_pred_class = []\n","        val_loss_class = []\n","        val_targets = []\n","        test_pred_class = []\n","        test_targets = []\n","      \n","        batch_generator = iterator.get_batches(train_set, shuffle=True)\n","        batch_generator_2 = iterator.get_batches(valid_set, shuffle=True)\n","        batch_generator_3 = iterator.get_batches(test_set, shuffle=True)\n","        \n","        for inputs, targets, subjects in batch_generator:\n","            _, s_loss, subject_out = sess.run([subject_op, subject_loss, subject_model], feed_dict={X: inputs, mode:True, lr:0.0005, labels:subjects})\n","\n","            subject_logits = subject_out[0]\n","            sinc_feat = subject_out[1]\n","            \n","            prediction = np.argmax(subject_logits, 1)\n","            all_pred_subject.extend(prediction)\n","            all_loss_subject.append(s_loss)\n","            all_subjects.extend(subjects)\n","            \n","            _, c_loss, class_logits = sess.run([class_op, class_loss, class_model], feed_dict={X: inputs, Y: sinc_feat, mode:True, lr:0.001, labels:targets})\n","\n","  \n","            prediction = np.argmax(class_logits, 1)\n","            all_pred_class.extend(prediction)\n","            all_loss_class.append(c_loss)\n","            all_targets.extend(targets)\n","            \n","        \n","        subject_accuracy = np.sum(np.equal(all_pred_subject, all_subjects)) / float(len(all_pred_subject))\n","        class_accuracy = np.sum(np.equal(all_pred_class, all_targets)) / float(len(all_pred_class))\n","\n","        num_nodes = len([n.name for n in tf.get_default_graph().as_graph_def().node])\n","    \n","        print(\"Epoch {} nodes {}\".format(epoch, num_nodes))\n","        print(\"Subjects: Loss {:.6f}, Accuracy: {}, Kappa: {}\".format(np.mean(all_loss_subject), subject_accuracy, cohen_kappa_score(all_pred_subject, all_subjects)))\n","        print(\"Class:    Loss {:.6f}, Accuracy: {}, Kappa: {}\".format(np.mean(all_loss_class), class_accuracy, cohen_kappa_score(all_pred_class, all_targets)))\n","\n","        \n","        \n","        for inputs, targets, subjects in batch_generator_2:\n","            s_loss, subject_out = sess.run([subject_loss, subject_model], feed_dict={X: inputs, mode:False, labels:subjects})\n","\n","            subject_logits = subject_out[0]\n","            sinc_feat = subject_out[1]\n","            \n","            prediction = np.argmax(subject_logits, 1)\n","            val_pred_subject.extend(prediction)\n","            val_loss_subject.append(s_loss)\n","            val_subjects.extend(subjects)\n","\n","            c_loss, class_logits = sess.run([class_loss, class_model], feed_dict={X: inputs, Y: sinc_feat, mode:False, labels:targets})\n","\n","            prediction = np.argmax(class_logits, 1)\n","            val_pred_class.extend(prediction)\n","            val_loss_class.append(c_loss)\n","            val_targets.extend(targets)\n","\n","        class_accuracy = np.sum(np.equal(val_pred_class, val_targets)) / float(len(val_pred_class))\n","        subject_accuracy = np.sum(np.equal(val_pred_subject, val_subjects)) / float(len(val_pred_subject))\n","\n","        print(\"Subjects: Loss {:.6f}, Accuracy: {}, Kappa: {}\".format(np.mean(val_loss_subject), subject_accuracy, cohen_kappa_score(val_pred_subject, val_subjects)))\n","        print(\"Class:    Loss {:.6f}, Accuracy: {}, Kappa: {}\".format(np.mean(val_loss_class), class_accuracy, cohen_kappa_score(val_pred_class, val_targets)))\n","        \n","        \n","        \n","        for inputs, targets, subjects in batch_generator_3:\n","            sinc_feat = sess.run(sincnet, feed_dict={X: inputs, mode:False})\n","            \n","            c_loss, class_logits = sess.run([class_loss, class_model], feed_dict={X: inputs, Y: sinc_feat, mode:False, labels:targets})\n","\n","            prediction = np.argmax(class_logits, 1)\n","            test_pred_class.extend(prediction)\n","            test_targets.extend(targets)\n","\n","        class_accuracy = np.sum(np.equal(test_pred_class, test_targets)) / float(len(test_pred_class))\n","            \n","          \n","        if class_accuracy > max_test_acc:\n","            max_test_acc = class_accuracy\n","        \n","        print(\"TestC:    Loss {:.6f}, Accuracy: {}, max_test_acc: {}, Kappa: {}\".format(c_loss, class_accuracy, max_test_acc, cohen_kappa_score(test_pred_class, test_targets)))\n","\n","#         Save checkpoints\n","#         saver.save(sess, \"/content/gdrive/My Drive/checkpoints/sincnet_reversal/\" + str(test_subject+1) + \"/\" + str(epoch) + \"/model.ckpt\" )\n","        \n","    print(\"{} Done training...\".format(datetime.now()))\n","\n","\n"],"execution_count":0,"outputs":[]}]}