{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CSC494_MFCC.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"colab_type":"code","id":"nkZkxUon92Bx","colab":{}},"cell_type":"code","source":["# Google drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GbIG9CJq_28t","colab_type":"code","colab":{}},"cell_type":"code","source":["!pip install mne\n","!pip install python_speech_features\n","\n","from sklearn.preprocessing import normalize\n","from collections import OrderedDict\n","import numpy as np\n","import python_speech_features\n","import tensorflow as tf\n","from datetime import datetime\n","import tensorflow as tf\n","from load_data import BCICompetition4Set2A\n","import helper_functions as hf\n","from sklearn.metrics import cohen_kappa_score\n","import pickle"],"execution_count":0,"outputs":[]},{"metadata":{"id":"L_2-_Avp_d6Z","colab_type":"code","colab":{}},"cell_type":"code","source":["device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KPjbQjMRMn8l","colab_type":"code","colab":{}},"cell_type":"code","source":["# Data helper functions\n","test_subject = 9\n","test_subject -= 1\n","r = 9\n","\n","def BCI_read_data(filename, labels_filename):    \n","    train_loader = BCICompetition4Set2A(filename=filename, labels_filename=labels_filename)\n","    train_cnt = train_loader.load()\n","    train_cnt = train_cnt.drop_channels(['STI 014', 'EOG-left', 'EOG-central', 'EOG-right'])\n","    assert len(train_cnt.ch_names) == 22\n","    \n","    train_cnt = hf.mne_apply(lambda a: a * 1e6, train_cnt)\n","    train_cnt = hf.mne_apply(lambda a: hf.bandpass_cnt(a, 0, 38.0, train_cnt.info['sfreq'],\n","                               filt_order=3,\n","                               axis=1), train_cnt)\n","    train_cnt = hf.mne_apply(lambda a: hf.exponential_running_standardize(a.T, factor_new=1e-3,\n","                                                  init_block_size=1000,\n","                                                  eps=1e-4).T, train_cnt)\n","    \n","    return train_cnt\n","\n","def BCI_load_all_data():\n","    ival = [-500, 4000]\n","    marker_def = OrderedDict([('Left Hand', [1]), ('Right Hand', [2],),\n","                              ('Foot', [3]), ('Tongue', [4])])\n","    \n","    raw_files = [BCI_read_data(\"A0\" + str(f) + \"T.gdf\", \n","                               \"A0\" + str(f) + \"T.mat\") for f in range(1, r+1)]\n","    \n","    \n","    train_set_list = [hf.create_signal_target_from_raw_mne(raw, marker_def, ival) for raw in raw_files]\n","    train_set_X = np.concatenate([train_set_list[i].X for i in  range(r) if i!=test_subject])\n","    # train_set_X = np.concatenate([train_set.X for train_set in train_set_list[:-1]])\n","    train_set_y = np.concatenate([train_set_list[i].y for i in  range(r) if i!=test_subject])\n","    # train_set_y = np.concatenate([train_set.y for train_set in train_set_list[:-1]])\n","    train_set_z = np.concatenate([np.zeros((288)) + i for i in range(r-1)])\n","    \n","    train_set_X = np.stack([np.stack([np.array(python_speech_features.mfcc(channel, 250, numcep=25)) for channel in event]) for event in train_set_X])\n","    \n","    indices = np.arange(train_set_y.shape[0])\n","    np.random.shuffle(indices)\n","    \n","    train_set = hf.SignalAndTarget(train_set_X[indices, :, :], train_set_y[indices], train_set_z[indices])\n","    train_set, valid_set = hf.split_into_two_sets(train_set, first_set_fraction=1-0.2)\n","    \n","    test_set_X = train_set_list[test_subject].X\n","    test_set_y = train_set_list[test_subject].y\n","    test_set_X = np.stack([np.stack([np.array(python_speech_features.mfcc(channel, 250, numcep=25)) for channel in event]) for event in test_set_X])\n","    test_set = hf.SignalAndTarget(test_set_X, test_set_y)\n","    \n","    iterator = hf.CropsFromTrialsIterator(batch_size=60, input_time_length=13, n_preds_per_input=4)\n","    \n","    return iterator, train_set, valid_set, test_set"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OrWHGW3DMraw","colab_type":"code","colab":{}},"cell_type":"code","source":["iterator, train_set, valid_set, test_set = BCI_load_all_data()\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zZaGuQqt8n9S","colab_type":"code","colab":{}},"cell_type":"code","source":["# iterator = hf.CropsFromTrialsIterator(batch_size=60, input_time_length=374, n_preds_per_input=4)\n","# train_set = np.load(\"/content/gdrive/My Drive/data/train_set_374_13.npy\").item(0)\n","# valid_set = np.load(\"/content/gdrive/My Drive/data/valid_set_374_13.npy\").item(0)\n","# test_set = np.load(\"/content/gdrive/My Drive/data/test_set_374_13.npy\").item(0)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UMqaQNLKNf6j","colab_type":"code","colab":{}},"cell_type":"code","source":["def shallow_brain_model(features, mode):\n","    conv1 = tf.layers.conv2d(inputs=features, filters = 20, kernel_size=(1, 13), strides=(1, 3), activation=None)\n","    batch1 = tf.layers.batch_normalization(conv1, momentum=0.1)\n","    activation1 = tf.nn.relu(batch1)\n","    dropout1 = tf.layers.dropout(activation1, 0.5, training=mode)\n","    \n","    conv2 = tf.layers.conv2d(inputs=dropout1, filters = 10, kernel_size=(8, 2), strides=(3, 1), activation=None)\n","    batch2 = tf.layers.batch_normalization(conv2, momentum=0.1)\n","    activation2 = tf.nn.relu(batch2)\n","    dropout2 = tf.layers.dropout(activation2, 0.5, training=mode)\n","    \n","    conv3 = tf.layers.conv2d(inputs=dropout2, filters = 8, kernel_size=(1, 15), strides=4, activation=None)\n","    batch3 = tf.layers.batch_normalization(conv3, momentum=0.1)\n","    activation3 = tf.nn.relu(batch3)\n","    \n","    conv4 = tf.layers.conv2d(inputs=activation3, filters = 4, kernel_size=(2, 27), strides=1, activation=None)\n","    \n","    logits = tf.reshape(conv4, [-1, 4])\n","\n","    return logits\n","  \n","def detect_subject_model(features, mode):\n","    conv1 = tf.layers.conv2d(inputs=features, filters = 20, kernel_size=(1, 13), strides=(1, 3), activation=None)\n","    batch1 = tf.layers.batch_normalization(conv1, momentum=0.1)\n","    activation1 = tf.nn.relu(batch1)\n","    dropout1 = tf.layers.dropout(activation1, 0.5, training=mode)\n","    \n","    conv2 = tf.layers.conv2d(inputs=dropout1, filters = 10, kernel_size=(8, 2), strides=(3, 1), activation=None)\n","    batch2 = tf.layers.batch_normalization(conv2, momentum=0.1)\n","    activation2 = tf.nn.relu(batch2)\n","    dropout2 = tf.layers.dropout(activation2, 0.5, training=mode)\n","    \n","    conv3 = tf.layers.conv2d(inputs=dropout2, filters = 8, kernel_size=(1, 15), strides=4, activation=None)\n","    batch3 = tf.layers.batch_normalization(conv3, momentum=0.1)\n","    activation3 = tf.nn.relu(batch3)\n","    \n","    conv4 = tf.layers.conv2d(inputs=activation3, filters = 8, kernel_size=(2, 27), strides=1, activation=None)\n","    \n","    logits = tf.reshape(conv4, [-1, 8])\n","\n","    return logits\n","\n","def mask_model(features, mode):\n","    conv1 = tf.layers.conv2d(inputs=features, filters=7, kernel_size=(15, 23), strides=(1, 3), activation=None)\n","    batch1 = tf.layers.batch_normalization(conv1, momentum=0.1)\n","    activation1 = tf.nn.elu(batch1)\n","    dropout1 = tf.layers.dropout(activation1, 0.5, training=mode)\n","    \n","    conv2 = tf.layers.conv2d_transpose(inputs=dropout1, filters=13, kernel_size=(15, 23), strides=(1, 3), activation=None)\n","    batch2 = tf.layers.batch_normalization(conv2, momentum=0.1)\n","    activation2 = tf.nn.elu(batch2)\n","    \n","    return activation2"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5NHlNnkVP-bS","colab_type":"code","colab":{}},"cell_type":"code","source":["tf.reset_default_graph()\n","\n","# MFCC_features = tf.placeholder(tf.float32, shape=[60, 22, 374, 13])\n","MFCC_features = tf.placeholder(tf.float32, shape=[None, None, None, 25])\n","# mask = tf.placeholder(tf.float32, shape=[None, None, None, 13])\n","labels = tf.placeholder(tf.int32, shape=[None])\n","lr = tf.placeholder(tf.float32)\n","mode = tf.placeholder(tf.bool)\n","\n","m_model = mask_model(MFCC_features, mode)\n","subject_model = detect_subject_model(m_model, mode)\n","class_model = shallow_brain_model(MFCC_features, mode)\n","\n","if tf.train.get_global_step() is None:\n","    tf.train.create_global_step()\n","    \n","print(np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()]))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KdoxIqMqQU7a","colab_type":"code","colab":{}},"cell_type":"code","source":["subject_loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=subject_model)\n","class_loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=class_model)\n","\n","subject_optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n","class_optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n","\n","subject_op = subject_optimizer.minimize(loss=subject_loss, global_step=tf.train.get_global_step())\n","class_op = class_optimizer.minimize(loss=class_loss)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"mj65c3QxT2c6","colab":{}},"cell_type":"code","source":["epoch_number = 600\n","\n","with tf.Session() as sess:\n","  \n","    sess.run(tf.global_variables_initializer())\n","    \n","    print(\"{} Start training...\".format(datetime.now()))\n","    \n","    for epoch in range(epoch_number):\n","      \n","        all_pred_subject = []\n","        all_loss_subject = []\n","        all_subjects = []\n","        val_pred_subject = []\n","        val_loss_subject = []\n","        val_subjects = []\n","        \n","        \n","        all_pred_class = []\n","        all_loss_class = []\n","        all_targets = []\n","        val_pred_class = []\n","        val_loss_class = []\n","        val_targets = []\n","        test_pred_class = []\n","        test_targets = []\n","      \n","        batch_generator = iterator.get_batches(train_set, shuffle=True)\n","        batch_generator_2 = iterator.get_batches(valid_set, shuffle=True)\n","        batch_generator_3 = iterator.get_batches(test_set, shuffle=True)\n","        \n","        for inputs, targets, subjects in batch_generator:\n","#             _, s_loss, subject_logits = sess.run([subject_op, subject_loss, subject_model], feed_dict={MFCC_features: inputs, mode:True, lr:0.0001, labels:subjects})\n","            \n","#             prediction = np.argmax(subject_logits, 1)\n","#             all_pred_subject.extend(prediction)\n","#             all_loss_subject.append(s_loss)\n","#             all_subjects.extend(subjects)\n","\n","            _, c_loss, class_logits = sess.run([class_op, class_loss, class_model], feed_dict={MFCC_features: inputs, mode:True, lr:0.001, labels:targets})\n","            \n","            prediction = np.argmax(class_logits, 1)\n","            all_pred_class.extend(prediction)\n","            all_loss_class.append(c_loss)\n","            all_targets.extend(targets)\n","            \n","            \n","#         subject_accuracy = np.sum(np.equal(all_pred_subject, all_subjects)) / float(len(all_pred_subject))\n","        class_accuracy = np.sum(np.equal(all_pred_class, all_targets)) / float(len(all_pred_class))\n","        \n","        num_nodes = len([n.name for n in tf.get_default_graph().as_graph_def().node])\n","\n","        print(\"Epoch {} nodes {}\".format(epoch, num_nodes))\n","#         print(\"Subjects: Loss {:.6f}, Accuracy: {}, Kappa {}\".format(np.mean(all_loss_subject), subject_accuracy, cohen_kappa_score(all_pred_subject, all_subjects)))\n","        print(\"Class:    Loss {:.6f}, Accuracy: {}, Kappa {}\".format(np.mean(all_loss_class), class_accuracy, cohen_kappa_score(all_pred_class, all_targets)))\n","\n","        \n","        \n","        \n","        \n","        \n","        for inputs, targets, subjects in batch_generator_2:\n","#             s_loss, subjects_logits = sess.run([subject_loss, subject_model], feed_dict={MFCC_features: inputs, mode:False, labels:subjects})\n","            \n","#             prediction = np.argmax(subjects_logits, 1)\n","#             val_pred_subject.extend(prediction)\n","#             val_loss_subject.append(s_loss)\n","#             val_subjects.extend(subjects)\n","            \n","            \n","            c_loss, class_logits = sess.run([class_loss, class_model], feed_dict={MFCC_features: inputs, mode:False, labels:targets})\n","\n","            prediction = np.argmax(class_logits, 1)\n","            val_pred_class.extend(prediction)\n","            val_loss_class.append(c_loss)\n","            val_targets.extend(targets)\n","\n","        class_accuracy = np.sum(np.equal(val_pred_class, val_targets)) / float(len(val_pred_class))\n","#         subject_accuracy = np.sum(np.equal(val_pred_subject, val_subjects)) / float(len(val_pred_subject))\n","\n","          \n","        print(\"Class:    Loss {:.6f}, Accuracy: {}, Kappa: {}\".format(np.mean(val_loss_class), class_accuracy, cohen_kappa_score(val_pred_class, val_targets)))\n","#         print(\"Subjects: Loss {:.6f}, Accuracy: {}, Kappa: {}\".format(np.mean(val_loss_subject), subject_accuracy, cohen_kappa_score(val_pred_subject, val_subjects)))\n","        \n","        \n","        \n","        \n","        \n","        \n","#         for inputs, targets, subjects in batch_generator_3:\n","#             c_loss, class_logits = sess.run([class_loss, class_model], feed_dict={MFCC_features: inputs, mode:False, labels:targets})\n","\n","#             prediction = np.argmax(class_logits, 1)\n","#             test_pred_class.extend(prediction)\n","#             test_targets.extend(targets)\n","\n","#         class_accuracy = np.sum(np.equal(test_pred_class, test_targets)) / float(len(test_pred_class))\n","\n","#         print(\"TestC:    Loss {:.6f}, Accuracy: {}, Kappa: {}\".format(c_loss, class_accuracy, cohen_kappa_score(test_pred_class, test_targets)))\n"],"execution_count":0,"outputs":[]}]}