{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CSC494_Gradient_Reversal.ipynb","version":"0.3.2","provenance":[],"private_outputs":true,"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python2","display_name":"Python 2"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"u0ShW_HhU10A","colab_type":"code","colab":{}},"cell_type":"code","source":["#TensorBoard on colab\n","!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","!unzip ngrok-stable-linux-amd64.zip\n","\n","LOG_DIR = './log'\n","get_ipython().system_raw(\n","    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n","    .format(LOG_DIR)\n",")\n","\n","get_ipython().system_raw('./ngrok http 6006 &')\n","\n","! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""],"execution_count":0,"outputs":[]},{"metadata":{"id":"3ItMRqnwqNy6","colab_type":"code","colab":{}},"cell_type":"code","source":["# Google drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"L92FZ2qfCJLC","colab_type":"code","colab":{}},"cell_type":"code","source":["!pip install mne\n","\n","from collections import OrderedDict\n","import numpy as np\n","import tensorflow as tf\n","from datetime import datetime\n","from sklearn.metrics import cohen_kappa_score\n","from mne.io import read_raw_edf, find_edf_events\n","# from flip_gradient import flip_gradient\n","# import helper_functions as hf\n","# from load_data import BCICompetition4Set2A\n","import mne"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4hhnkESkCiSJ","colab_type":"code","colab":{}},"cell_type":"code","source":["device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OFQfHhpye98N","colab_type":"code","colab":{}},"cell_type":"code","source":["# Data helper functions\n","test_subject = 9\n","test_subject -= 1\n","\n","def BCI_read_data(filename, labels_filename):    \n","    train_loader = BCICompetition4Set2A(filename=filename, labels_filename=labels_filename)\n","    train_cnt = train_loader.load()\n","    train_cnt = train_cnt.drop_channels(['STI 014', 'EOG-left', 'EOG-central', 'EOG-right'])\n","    assert len(train_cnt.ch_names) == 22\n","    \n","    train_cnt = hf.mne_apply(lambda a: a * 1e6, train_cnt)\n","    train_cnt = hf.mne_apply(lambda a: hf.bandpass_cnt(a, 0, 38.0, train_cnt.info['sfreq'],\n","                               filt_order=3,\n","                               axis=1), train_cnt)\n","    train_cnt = hf.mne_apply(lambda a: hf.exponential_running_standardize(a.T, factor_new=1e-3,\n","                                                  init_block_size=1000,\n","                                                  eps=1e-4).T, train_cnt)\n","    \n","    print(\"$$$$$$$$$$$$$$$$$$\")\n","    print(train_cnt.info['sfreq'])\n","    print(\"$$$$$$$$$$$$$$$$$$\")\n","    \n","    \n","    return train_cnt\n","\n","def BCI_load_all_data():\n","    ival = [-500, 4000]\n","    marker_def = OrderedDict([('Left Hand', [1]), ('Right Hand', [2],),\n","                              ('Foot', [3]), ('Tongue', [4])])\n","    \n","    raw_files = [BCI_read_data(\"A0\" + str(f) + \"T.gdf\", \n","                               \"A0\" + str(f) + \"T.mat\") for f in range(1, 10)]\n","    \n","    \n","    train_set_list = [hf.create_signal_target_from_raw_mne(raw, marker_def, ival) for raw in raw_files]\n","    \n","    train_set_X = np.concatenate([train_set_list[i].X for i in  range(9) if i!=test_subject])\n","    # train_set_X = np.concatenate([train_set.X for train_set in train_set_list[:-1]])\n","    train_set_y = np.concatenate([train_set_list[i].y for i in  range(9) if i!=test_subject])\n","    # train_set_y = np.concatenate([train_set.y for train_set in train_set_list[:-1]])\n","    train_set_z = np.concatenate([np.zeros((288)) + i for i in range(8)])\n","    \n","    indices = np.arange(train_set_y.shape[0])\n","    np.random.shuffle(indices)\n","    \n","    train_set = hf.SignalAndTarget(train_set_X[indices, :, :], train_set_y[indices], train_set_z[indices])\n","    test_set = train_set_list[test_subject]\n","    train_set, valid_set = hf.split_into_two_sets(train_set, first_set_fraction=1-0.2)\n","    \n","    iterator = hf.CropsFromTrialsIterator(batch_size=60, input_time_length=1125, n_preds_per_input=4)\n","    \n","    return iterator, train_set, valid_set, test_set, raw_files[0].info['sfreq']"],"execution_count":0,"outputs":[]},{"metadata":{"id":"23EtYwkPmuj_","colab_type":"code","colab":{}},"cell_type":"code","source":["iterator, train_set, valid_set, test_set, s = BCI_load_all_data()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"F-wcY9U6D5pZ","colab_type":"code","colab":{}},"cell_type":"code","source":["def shallow_brain_model(features, mode):\n","    conv1 = tf.layers.conv2d(inputs=features, filters = 40, kernel_size=(1, 25), strides=1, activation=None)\n","\n","    conv2 = tf.layers.conv2d(inputs=conv1, filters = 40, kernel_size=(18, 1), strides=1, activation=None)\n","    batch2 = tf.layers.batch_normalization(conv2, momentum=0.1)\n","    activation2 = tf.nn.elu(batch2)\n","    pool2 = tf.layers.max_pooling2d(inputs=activation2, pool_size=(1, 80), strides=15)\n","    dropout2 = tf.layers.dropout(pool2, 0.5, training=mode)\n","\n","    conv3 = tf.layers.conv2d(inputs=dropout2, filters = 4, kernel_size=(1, 69), strides=1, activation=None)\n","    logits = tf.reshape(conv3, [-1, 4])\n","\n","    return logits\n","\n","def detect_subject_model(features):\n","    print(features.shape)\n","    feat = flip_gradient(features)\n","\n","    conv1 = tf.layers.conv2d(inputs=feat, filters = 40, kernel_size=(1, 25), strides=1, activation=None)\n","\n","    conv2 = tf.layers.conv2d(inputs=conv1,filters = 40, kernel_size=(18, 1), strides=1, activation=None)\n","    batch2 = tf.layers.batch_normalization(conv2, momentum=0.1)\n","    activation2 = tf.nn.elu(batch2)\n","    pool2 = tf.layers.max_pooling2d(inputs=activation2, pool_size=(1, 80), strides=15)\n","\n","    conv3 = tf.layers.conv2d(inputs=pool2, filters = 8, kernel_size=(1, 55), strides=1, activation=None)\n","    logits = tf.reshape(conv3, [-1, 8])\n","\n","    print(logits.shape)\n","    return logits\n","  \n","def embed_model(features):\n","    conv1 = tf.layers.conv2d(inputs=features, filters=1, kernel_size=(3, 100), strides=1, activation=None)\n","    batch1 = tf.layers.batch_normalization(conv1, momentum=0.1)\n","    activation1 = tf.nn.elu(batch1)\n","\n","    conv2 = tf.layers.conv2d(inputs=activation1, filters=1, kernel_size=(3, 100), strides=1, activation=None)\n","    batch2 = tf.layers.batch_normalization(conv2, momentum=0.1)\n","    activation2 = tf.nn.elu(batch2)\n","    \n","    conv3t = tf.layers.conv2d_transpose(inputs=activation2, filters=1, kernel_size=(3, 100), strides=1, activation=None)\n","    batch3 = tf.layers.batch_normalization(conv3t, momentum=0.1)\n","    activation3 = tf.nn.elu(batch3)\n","    \n","    conv4t =tf.layers.conv2d_transpose(inputs=activation3, filters=1, kernel_size=(3, 100), strides=1, activation=None)\n","    batch4 = tf.layers.batch_normalization(conv4t, momentum=0.1)\n","    activation4 = tf.nn.elu(batch4)\n","    \n","    return [activation2, activation4]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4R9utlYTfKmG","colab_type":"code","colab":{}},"cell_type":"code","source":["# Remove tensorboard files from previous runs \n","# import shutil\n","# shutil.rmtree(\"./log\") \n","\n","# Reset graph\n","tf.reset_default_graph()\n","\n","# Placeholders\n","# input_data = tf.placeholder(tf.float32, shape=[60, 22, 1125, 1])\n","\n","input_data = tf.placeholder(tf.float32, shape=[None, None, None, 1])\n","rec_l = tf.placeholder(tf.float32, shape=[None, None, None, 1])\n","labels = tf.placeholder(tf.int32, shape=[None])\n","mode = tf.placeholder(tf.bool)\n","lr = tf.placeholder(tf.float32)\n","\n","\n","# Add new models to the graph\n","# embedding_model = embed_model(input_data)\n","# detection_model = detect_subject_model(embedding_model[0])\n","# class_model = shallow_brain_model(embedding_model[0], mode)\n","class_model = shallow_brain_model(input_data, mode)\n","# reconstruction = embedding_model[1]\n","\n","\n","print(np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()]))\n","\n","if tf.train.get_global_step() is None:\n","    tf.train.create_global_step()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"AOThHTnat4QF","colab_type":"code","colab":{}},"cell_type":"code","source":["# subject_loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=detection_model)\n","class_loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=class_model) \n","# reconstruction_loss = tf.losses.mean_squared_error(labels=rec_l, predictions=reconstruction)\n","\n","# subject_loss_summ = tf.summary.scalar(\"Subject_loss\", subject_loss)\n","class_loss_summ = tf.summary.scalar(\"Class_loss\", class_loss)\n","# reconstruction_loss_summ = tf.summary.scalar(\"Reconstruction_loss\", reconstruction_loss)\n","\n","class_optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n","# subject_optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n","# reconstruction_optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n","\n","# subject_op = subject_optimizer.minimize(loss=subject_loss, global_step=tf.train.get_global_step())\n","class_op = class_optimizer.minimize(loss=class_loss)\n","# reconstruction_op = reconstruction_optimizer.minimize(loss=reconstruction_loss)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XQiJof96FhH5","colab_type":"code","colab":{}},"cell_type":"code","source":["epoch_number = 30\n","max_test_acc = 0\n","min_val_loss = 100\n","\n","with tf.Session() as sess:\n","#     writer_train = tf.summary.FileWriter(LOG_DIR + \"/train\")\n","#     writer_train.add_graph(sess.graph)\n","    \n","#     writer_val = tf.summary.FileWriter(LOG_DIR + \"/val\")\n","#     writer_val.add_graph(sess.graph)\n","    \n","\n","    sess.run(tf.global_variables_initializer())\n","    saver = tf.train.Saver()\n","\n","    # Restore checkpoints\n","#     try:\n","#         saver.restore(sess, \"/content/gdrive/My Drive/checkpoints/last_term_model/\" + str(test_subject+1) + \"/\" + str(27) + \"/model.ckpt\")\n","#     except Exception as e:\n","#         print (e)\n","\n","    print(\"{} Start training...\".format(datetime.now()))\n","\n","    for epoch in range(epoch_number):\n","\n","        all_loss_rec = []\n","        val_loss_rec = []\n","        \n","        all_pred_class = []\n","        all_loss_class = []\n","        all_targets = []\n","        val_pred_class = []\n","        val_loss_class = []\n","        val_targets = []\n","        test_pred_class = []\n","        test_targets = []\n","        \n","        all_pred_subject = []\n","        all_loss_subject = []\n","        all_subjects = []\n","        val_pred_subject = []\n","        val_loss_subject = []\n","        val_subjects = []\n","        test_pred_subject = []\n","        \n","        batch_generator = iterator.get_batches(train_set, shuffle=True)\n","        batch_generator_2 = iterator.get_batches(valid_set, shuffle=True)\n","        batch_generator_3 = iterator.get_batches(test_set, shuffle=True)\n","        \n","        for inputs, targets, subjects in batch_generator:\n","# #             summ_rec, _, rec_loss, rec = sess.run([reconstruction_loss_summ, reconstruction_op, reconstruction_loss, reconstruction], feed_dict={input_data: inputs, lr: 0.0005, rec_l: inputs})\n","#             _, rec_loss, rec = sess.run([reconstruction_op, reconstruction_loss, reconstruction], feed_dict={input_data: inputs, lr: 0.0005, rec_l: inputs})\n","\n","#             all_loss_rec.append(rec_loss)\n","            \n","#             summ_class, _, c_loss, class_logits = sess.run([class_loss_summ, class_op, class_loss, class_model], feed_dict={input_data: inputs, mode:True, lr:0.001, labels:targets})\n","            _, c_loss, class_logits = sess.run([class_op, class_loss, class_model], feed_dict={input_data: inputs, mode:True, lr:0.001, labels:targets})\n","            \n","            prediction = np.argmax(class_logits, 1)\n","            all_pred_class.extend(prediction)\n","            all_loss_class.append(c_loss)\n","            all_targets.extend(targets)\n","            \n","# #             summ_subject, _, s_loss, subject_logits = sess.run([subject_loss_summ, subject_op, subject_loss, detection_model], feed_dict={input_data: inputs, lr:0.00005, labels:subjects})\n","#             _, s_loss, subject_logits = sess.run([subject_op, subject_loss, detection_model], feed_dict={input_data: inputs, lr:0.00005, labels:subjects})\n","\n","#             if tf.train.get_global_step().eval() % 31 == 0:\n","#                 writer_train.add_summary(summ_rec, epoch)\n","#                 writer_train.add_summary(summ_class, epoch)\n","#                 writer_train.add_summary(summ_subject, epoch)\n","#                 writer_train.flush()\n","            \n","#             prediction = np.argmax(subject_logits, 1)\n","#             all_pred_subject.extend(prediction)\n","#             all_loss_subject.append(s_loss)\n","#             all_subjects.extend(subjects)\n","\n","        class_accuracy = np.sum(np.equal(all_pred_class, all_targets)) / float(len(all_pred_class))\n","#         subject_accuracy = np.sum(np.equal(all_pred_subject, all_subjects)) / float(len(all_pred_subject))\n","\n","        num_nodes = len([n.name for n in tf.get_default_graph().as_graph_def().node])\n","\n","        print(\"Epoch {} nodes {}\".format(epoch, num_nodes))\n","        print(\"Class:    Loss {:.6f}, Accuracy: {}, Kappa {}\".format(np.mean(all_loss_class), class_accuracy, cohen_kappa_score(all_pred_class, all_targets)))\n","#         print(\"Subjects: Loss {:.6f}, Accuracy: {}, Kappa {}\".format(np.mean(all_loss_subject), subject_accuracy, cohen_kappa_score(all_pred_subject, all_subjects)))\n","#         print(\"Reconstruction: Loss {:.6}\".format(np.mean(all_loss_rec)))\n","\n","        for inputs, targets, subjects in batch_generator_2:\n","# #             summ_rec, rec_loss, rec = sess.run([reconstruction_loss_summ, reconstruction_loss, reconstruction], feed_dict={input_data: inputs ,rec_l: inputs})\n","#             rec_loss, rec = sess.run([reconstruction_loss, reconstruction], feed_dict={input_data: inputs ,rec_l: inputs})\n","            \n","#             val_loss_rec.append(rec_loss)\n","            \n","#             summ_class, c_loss, class_logits = sess.run([class_loss_summ, class_loss, class_model], feed_dict={input_data: inputs, mode:False, labels:targets})\n","            c_loss, class_logits = sess.run([class_loss, class_model], feed_dict={input_data: inputs, mode:False, labels:targets})\n","\n","            prediction = np.argmax(class_logits, 1)\n","            val_pred_class.extend(prediction)\n","            val_loss_class.append(c_loss)\n","            val_targets.extend(targets)\n","            \n","# #             summ_subject, s_loss, subjects_logits = sess.run([subject_loss_summ, subject_loss, detection_model], feed_dict={input_data: inputs, labels:subjects})\n","#             s_loss, subjects_logits = sess.run([subject_loss, detection_model], feed_dict={input_data: inputs, labels:subjects})\n","            \n","#             if len(val_loss_subject) == 0:\n","#                 writer_val.add_summary(summ_rec, epoch)\n","#                 writer_val.add_summary(summ_class, epoch)\n","#                 writer_val.add_summary(summ_subject, epoch)\n","#                 writer_val.flush()\n","            \n","#             prediction = np.argmax(subjects_logits, 1)\n","#             val_pred_subject.extend(prediction)\n","#             val_loss_subject.append(s_loss)\n","#             val_subjects.extend(subjects)\n","\n","        class_accuracy = np.sum(np.equal(val_pred_class, val_targets)) / float(len(val_pred_class))\n","#         subject_accuracy = np.sum(np.equal(val_pred_subject, val_subjects)) / float(len(val_pred_subject))\n","\n","        if min_val_loss > np.mean(val_loss_class):\n","            min_val_loss = np.mean(val_loss_class)\n","          \n","        print(\"Class:    Loss {:.6f}, min_loss: {}, Accuracy: {}, Kappa: {}\".format(np.mean(val_loss_class), min_val_loss, class_accuracy, cohen_kappa_score(val_pred_class, val_targets)))\n","#         print(\"Subjects: Loss {:.6f}, Accuracy: {}, Kappa: {}\".format(np.mean(val_loss_subject), subject_accuracy, cohen_kappa_score(val_pred_subject, val_subjects)))\n","#         print(\"Reconstruction: Loss {:.6}\".format(np.mean(val_loss_rec)))\n","\n","        for inputs, targets, subjects in batch_generator_3:\n","            c_loss, class_logits = sess.run([class_loss, class_model], feed_dict={input_data: inputs, mode:False, labels:targets})\n","\n","            prediction = np.argmax(class_logits, 1)\n","            test_pred_class.extend(prediction)\n","            test_targets.extend(targets)\n","\n","        class_accuracy = np.sum(np.equal(test_pred_class, test_targets)) / float(len(test_pred_class))\n","\n","        if class_accuracy > max_test_acc:\n","            max_test_acc = class_accuracy\n","            \n","        print(\"TestC:    Loss {:.6f}, Accuracy: {}, Max acc: {}, Kappa: {}\".format(c_loss, class_accuracy, max_test_acc, cohen_kappa_score(test_pred_class, test_targets)))\n","\n","#         Save checkpoints\n","#         saver.save(sess, \"/content/gdrive/My Drive/checkpoints/baseline_model/\" + str(test_subject+1) + \"/\" + str(epoch) + \"/model.ckpt\" )\n","        \n","    print(\"{} Done training...\".format(datetime.now()))\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2bav5TTQZmHi","colab_type":"code","colab":{}},"cell_type":"code","source":["batch_generator = iterator.get_batches(train_set, shuffle=True)\n","count = 0\n","        \n","for inputs, targets, subjects in batch_generator:\n","  count += inputs.shape[0]\n","  \n","count\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"47kp_ixmjYFT","colab_type":"code","colab":{}},"cell_type":"code","source":["a = []\n","for i in range(4):\n","  a.append(np.load(\"b_\" + str(i) + \".npy\"))\n","  \n","a[0].shape\n","\n","\n","b = []\n","for i in range(4):\n","  b.append(np.load(\"g_\" + str(i) + \".npy\"))\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bVJkLgJqB7XE","colab_type":"code","colab":{}},"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","fig, ax = plt.subplots(2, 4, figsize=(16, 8))\n","\n","# plt.grid(True)\n","\n","montage = mne.channels.read_montage('biosemi128')\n","montage.selection = montage.selection[:22]\n","\n","info = mne.create_info(montage.ch_names[:22], 250.0, 'eeg', montage=montage)\n","\n","# for i in range(4):\n","mne.viz.plot_topomap(a[0].squeeze()[:, 500], pos=info, vmin=-1, vmax=1, show=False, axes=ax[0][0])\n","mne.viz.plot_topomap(a[1].squeeze()[:, 500], pos=info, vmin=-1, vmax=1, show=False, axes=ax[0][1])\n","mne.viz.plot_topomap(a[2].squeeze()[:, 500], pos=info, vmin=-1, vmax=1, show=False, axes=ax[0][2])\n","mne.viz.plot_topomap(a[3].squeeze()[:, 500], pos=info, vmin=-1, vmax=1, show=False, axes=ax[0][3])\n","\n","d = mne.viz.plot_topomap(b[0].squeeze()[:, 500], pos=info, vmin=-1, vmax=1, show=False, axes=ax[1][0])\n","mne.viz.plot_topomap(b[1].squeeze()[:, 500], pos=info, vmin=-1, vmax=1, show=False, axes=ax[1][1])\n","mne.viz.plot_topomap(b[2].squeeze()[:, 200], pos=info, vmin=-1, vmax=1, show=False, axes=ax[1][2])\n","mne.viz.plot_topomap(b[3].squeeze()[:, 200], pos=info, vmin=-1, vmax=1, show=False, axes=ax[1][3])\n","\n","# fig.colorbar(d, ax=ax[1][0])\n","\n","plt.show"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cGduQ6w5FDWw","colab_type":"code","colab":{}},"cell_type":"code","source":["test_d = None\n","batch_generator = iterator.get_batches(train_set, shuffle=True)\n","batch_generator_2 = iterator.get_batches(valid_set, shuffle=True)\n","batch_generator_3 = iterator.get_batches(test_set, shuffle=True)\n","\n","for inputs, targets, subjects in batch_generator:\n","  test_d = inputs[0, :, :, 0]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tJEO7mxVFNRx","colab_type":"code","colab":{}},"cell_type":"code","source":["a = tf.global_variables()[:6]\n","\n","a.extend((tf.global_variables()[8:10]))\n","a"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qsnrhWbVjc3o","colab_type":"code","colab":{}},"cell_type":"code","source":["np.max(test_d[:, 1000])\n","np.min(test_d[:, 1000])"],"execution_count":0,"outputs":[]}]}